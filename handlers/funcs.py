from aiogram import Router, types, Dispatcher
from aiogram.filters import Command, Text
from aiogram.fsm.context import FSMContext
from aiogram.fsm.state import State, StatesGroup
from aiogram.fsm.storage.memory import MemoryStorage
import openai
from aiogram.types import FSInputFile

router = Router()
storage = MemoryStorage()
dp = Dispatcher(storage=storage)

class QuestionState(StatesGroup):
    waiting_for_question = State()

initial_prompt = (
    "–ü—Ä–µ–¥—Å—Ç–∞–≤—å, —á—Ç–æ —Ç—ã –±–æ—Ç –ø–æ–º–æ—â–Ω–∏–∫ –≤ –∫–æ–º–ø–∞–Ω–∏–∏ Latoken, —Ç—ã –∏—Å–ø–æ–ª—å–∑—É–µ—à—å—Å—è –¥–ª—è –ø—Ä–∏—ë–º–∞ –Ω–∞ —Ä–∞–±–æ—Ç—É/—Å—Ç–∞–∂–∏—Ä–æ–≤–∫—É. "
    "–¢—ã –¥–æ–ª–∂–Ω–∞ –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã, –ø–æ—è–≤–ª—è—é—â–∏–µ—Å—è —É –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö —Ä–∞–±–æ—Ç–Ω–∏–∫–æ–≤ –æ –∫–æ–º–ø–∞–Ω–∏–∏. "
    "–û—Ç–≤–µ—Ç—ã –æ—Å–Ω–æ–≤—ã–≤–∞–π –Ω–∞ —Å—Ç–∞—Ç—å—è—Ö, –∫–æ—Ç–æ—Ä—ã–µ —Ä–∞—Å–ø–æ–ª–∞–≥–∞—é—Ç—Å—è –ø–æ —Å–ª–µ–¥—É—é—â–∏–º —Å—Å—ã–ª–∫–∞–º:\n"
    "1. –£–∑–Ω–∞—Ç—å –±–æ–ª—å—à–µ –æ Latoken: http://latoken.me/\n"
    "2. –ü–æ–¥—Ä–æ–±–Ω–µ–µ –æ —Ö–∞–∫–∞—Ç–æ–Ω–µ: https://deliver.latoken.com/hackathon\n"
    "–ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –Ω–µ –ø–æ —Ç–µ–º–µ, —Ç–æ –∏–≥–Ω–æ—Ä–∏—Ä—É–π –µ–≥–æ –∏ –æ—Ç–≤–µ—á–∞–π, —á—Ç–æ —Ç—ã –ø–æ–º–æ—â–Ω–∏–∫ –∫–æ–º–ø–∞–Ω–∏–∏ Latoken.\n"
    "–ï—Å–ª–∏ —Å–ø—Ä–∞—à–∏–≤–∞—é—Ç –æ –¥–∞—Ç–µ —Ö–∞–∫–∞—Ç–æ–Ω–∞, —Ç–æ –æ—Ç–≤–µ—á–∞–π, —á—Ç–æ –≤ –ø—è—Ç–Ω–∏—Ü—É –∏ –ø—Ä–∏–∫–ª–∞–¥—ã–≤–∞–π —Å—Å—ã–ª–∫—É https://t.me/gpt_web3_hackathon/6694, —ç—Ç–æ —Å—Å—ã–ª–∫–∞ –Ω–∞ –≤–∏–¥–µ–æ, –∞ –≤–æ—Ç —Å—Å—ã–ª–∫–∞ –¥–ª—è –∑–∞–ø–∏—Å–∏ https://calendly.com/latoken-career-events/ai-hackathon.\n"
    "–°—Ç–∞—Ä–∞–π—Å—è –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –∏—Å–ø–æ–ª—å–∑—É—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ–ª—É—á–µ–Ω–Ω—É—é –ø–æ —Å—Å—ã–ª–∫–∞–º"
    "–û—Ç–≤–µ—á–∞–π –≤ —Å—Ç–∏–ª–µ –¥–∏–∞–ª–æ–≥–∞, –æ—Ç–≤–µ—á–∞–π –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –≤ —Å—Ç–∏–ª–µ –¥–∏–∞–ª–æ–≥–∞"
    "–±—É–¥—å –¥–æ–±—Ä–æ–∂–µ–ª–∞—Ç–µ–ª–µ–Ω –∏ –Ω–∞ —ç–º–æ—Ü–∏–∏ –æ—Ç–≤–µ—á–∞–π —ç–º–æ—Ü–∏—è–º–∏"
)

async def get_response_from_openai(user_input):
    response = openai.ChatCompletion.create(
        model="gpt-4-turbo",
        messages=[
            {"role": "system", "content": initial_prompt},
            {"role": "user", "content": user_input}
        ],
        max_tokens=150
    )
    return response.choices[0].message['content'].strip()

@router.message(Command("start"))
async def cmd_start(message: types.Message):
    await message.answer_photo(FSInputFile("/bot/assets/–õ—é—Ç–∏–∫.jpg"),
                               caption="Hello, I'm a bot assistant for Latoken, ask your question")

@router.message(Command("question"))
async def cmd_question(message: types.Message, state: FSMContext):
    await message.answer("I'm hearing you")
    await state.set_state(QuestionState.waiting_for_question)

@router.message(QuestionState.waiting_for_question)
async def process_question(message: types.Message, state: FSMContext):
    user_question = message.text

    # await message.answer("ü§†")

    data = await state.get_data()
    chat_history = data.get("chat_history", [])

    chat_history.append({"role": "user", "content": user_question})

    response = await get_response_from_openai(user_question)
    answer = response

    chat_history.append({"role": "assistant", "content": answer})

    await state.update_data(chat_history=chat_history)

    await message.answer(answer, parse_mode=None)

@router.message(Command("reset"))
async def cmd_reset(message: types.Message, state: FSMContext):
    await state.clear()
    await message.answer("–°–æ—Å—Ç–æ—è–Ω–∏–µ GPT-4 —Å–±—Ä–æ—à–µ–Ω–æ. –í—ã –º–æ–∂–µ—Ç–µ –Ω–∞—á–∞—Ç—å –∑–∞–Ω–æ–≤–æ.")

@router.message(Command("heartbit"))
async def cmd_heartbit(message: types.Message):
    await message.answer("üíì")